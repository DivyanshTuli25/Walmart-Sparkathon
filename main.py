# -- coding: utf-8 --
"""NEW PRODUCT DESC

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p-7cOHWD1ihvNJoR973TVp-I7V3vWT-D
"""

from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
import faiss
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import os
import torch
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_transformers import Html2TextTransformer
from langchain.document_loaders import AsyncChromiumLoader
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.llms import HuggingFacePipeline
from langchain.chains import LLMChain
import os
import re
import requests
# from google.colab import userdata
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from langchain.text_splitter import CharacterTextSplitter
from langchain.docstore.document import Document
from langchain import PromptTemplate, LLMChain
from langchain_core.runnables import RunnablePassthrough



file_path = 'new2.json'  # Replace with your file's path

temp1 = pd.read_json(file_path, lines=True)

file_path = 'delete.json'  # Replace with your file's path

temp2 = pd.read_json(file_path, lines=True)

df = pd.concat([temp1, temp2], ignore_index=True)

df.drop(columns=['question_id', 'asin'],inplace=True)
df_filtered = df[df['question_type'] != 'WH']

df_filtered['points']=df_filtered['bullet_point1']+df_filtered['bullet_point2']+df_filtered['bullet_point3']+df_filtered['bullet_point4']+df_filtered['bullet_point5']

df_filtered.drop(columns=['bullet_point1', 'bullet_point2', 'bullet_point3', 'bullet_point4', 'bullet_point5'],inplace=True)

def merge_answers(answer_list):
    return ' '.join(d['answer_text'] for d in answer_list)

df_filtered['merged_answer'] = df_filtered['answers'].apply(merge_answers)

df_filtered.drop(columns=['answers'],inplace=True)

df_filtered['all_detail']=df_filtered['product_description']+df_filtered['brand_name']+df_filtered['item_name']+df_filtered['points']

df_filtered.drop(columns=['product_description', 'brand_name', 'item_name', 'points', 'question_type'],inplace=True)



df_filtered['final'] = df_filtered['all_detail'] + " " + df_filtered['merged_answer']


df_filtered.drop(columns=['all_detail', 'merged_answer'],inplace=True)

df_final = pd.DataFrame(df_filtered['final'])

category_counts = df_filtered['answer_aggregated'].value_counts().sum()
category_counts_ = df_filtered['answer_aggregated'].value_counts()

print(category_counts)

category_counts_ = df_filtered['answer_aggregated'].value_counts()
print(category_counts_)



# @title answer_aggregated


# df_filtered.groupby('answer_aggregated').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
# plt.gca().spines[['top', 'right',]].set_visible(False)





df_final_sampled = df_final.sample(250)


def format_response (response: str) -> str:
    entries = re.split(r" (?<=]), (?=\[)", response)
    return [entry.strip("[]") for entry in entries]

os.environ ["GROQ_API_KEY"] = "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"

mistral_llm = ChatGroq(temperature = 0.2,model_name="llama3-8b-8192")

alldata=" "
for index, row in df_final_sampled.iterrows():
  alldata = alldata + " " +  row['final']





# Create a CharacterTextSplitter instance
text_splitter = CharacterTextSplitter(
    chunk_size=200,   # Size of each chunk
    chunk_overlap=1  # Overlap between chunks
)

# Split the text into chunks
chunks = text_splitter.split_text(alldata)

len(chunks)

len(chunks[0].split(" "))

def split_text_with_overlap(text, chunk_size=200, overlap=10):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += (chunk_size - overlap)
    return chunks

# Example usage
  # Replace with your actual string
chunks = split_text_with_overlap(alldata, chunk_size=200, overlap=10)

# Printing the chunks
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}:\n{chunk}\n")

class HuggingFaceEmbeddings:
    def _init_(self, model_name):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)

    def encode(self, texts):
        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True)
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.last_hidden_state.mean(dim=1).numpy()

# Instantiate the embeddings model
model_name = 'sentence-transformers/all-mpnet-base-v2'
hf_embeddings = HuggingFaceEmbeddings(model_name)

# Encode chunked documents
embeddings = hf_embeddings.encode([chunk for chunk in chunks])

# Create FAISS index
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)


class HuggingFaceEmbeddings:
    def _init_(self, model_name):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)

    def encode(self, texts):
        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True)
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.last_hidden_state.mean(dim=1).numpy()

    def embed_documents(self, documents):
        texts = [doc for doc in documents]
        return self.encode(texts)

    def embed_query(self, query):
        inputs = self.tokenizer(query, return_tensors='pt')
        outputs = self.model(**inputs)
        return outputs.last_hidden_state.mean(dim=1).detach().numpy()

model_name = 'sentence-transformers/all-mpnet-base-v2'
hf_embeddings = HuggingFaceEmbeddings(model_name)

type(chunk)

def retrieve(query, k=5):
    query_embedding = hf_embeddings.encode([query])
    distances, indices = index.search(query_embedding, 5)
    return [chunks[i] for i in indices[0]]

# Example retrieval
query = "How much is the battery life of laptop"
retrieved_docs = retrieve(query)
for doc in retrieved_docs:
    print(doc)
documents = [Document(page_content=chunk) for chunk in retrieved_docs]

db = FAISS.from_documents(documents, hf_embeddings)
retriever = db.as_retriever()

type(retrieved_docs[0])

print(documents[0].page_content)

def retrieve_chunks(query, index, hf_embeddings, documents, top_k=5):
    # Embed the query
    query_embedding = hf_embeddings.embed_query(query)

    # Search FAISS index
    distances, indices = index.search(query_embedding, top_k)

    # Check the lengths of indices and documents
    num_documents = len(documents)
    retrieved_chunks = []
    for idx in indices[0]:
        if idx < num_documents:
            retrieved_chunks.append(documents[idx].page_content)
        else:
            print(f"Index {idx} is out of bounds for documents list of length {num_documents}")

    return retrieved_chunks

# Example query
query = "what is the battery life of laptop"
retrieved_chunks = retrieve_chunks(query, index, hf_embeddings, documents)

# Print retrieved chunks
for i, chunk in enumerate(retrieved_chunks):
    print(f"Chunk {i+1}: {chunk}")

type(documents[0])

retriever = db.as_retriever()


# Define the prompt template
prompt_template = """
note while returning final answer please print little bit of context from docs that you have used to generate answer
### [INST] Instruction: Answer the question based on your docs knowledge. Here is context to help:

{context}

### QUESTION:
{question} [/INST]
"""

# Create prompt from prompt template
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template,
)

# Assume mistral_llm is defined elsewhere, and it's the language model you are using
# Create llm chain
llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)

# Define a retriever function that retrieves context given a query
def retriever(query):
    # Example implementation using the previously defined retrieve_chunks function
    # Ensure index, hf_embeddings, and documents are properly defined
    return retrieve_chunks(query, index, hf_embeddings, documents)

# Create RAG chain
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | llm_chain
)

# Invoke the RAG chain
result = rag_chain.invoke("What is the best laptop")

# Print the result
print(result)

text = result['text']

# Add line breaks to format the text as a paragraph
formatted_text = text.replace('\n', ' ')  # Replace existing line breaks with spaces
formatted_text = formatted_text.replace('. ', '.\n\n')  # Add double line breaks after periods

# Print the formatted text
print(formatted_text)